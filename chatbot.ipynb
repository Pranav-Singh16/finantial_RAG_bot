{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To access file from .env\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# To login Huggingface\n",
    "from huggingface_hub import login\n",
    "# JSON data loader\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "# Importing Embedding\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# To access LLM model\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace, HuggingFaceEmbeddings\n",
    "# connect with Pinecone\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Formatting data to store in PC\n",
    "import json\n",
    "from langchain.schema import Document\n",
    "from uuid import uuid4\n",
    "# prompt template\n",
    "from langchain.prompts import PromptTemplate\n",
    "# To format output\n",
    "from IPython.display import display, Markdown\n",
    "import pinecone\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from transformers import AutoTokenizer\n",
    "# for gradio\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = './env'\n",
    "\n",
    "load_dotenv(env)\n",
    "\n",
    "# Access the variables using the correct names\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_KEY\")\n",
    "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "from huggingface_hub import login\n",
    "login(HUGGINGFACE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "\n",
    "chat = ChatHuggingFace(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_pic_path = './image/bot.png'\n",
    "person_pic_path = './image/user.jpg'\n",
    "\n",
    "bot_pic = Image.open(bot_pic_path).convert('RGB').resize((100, 100))\n",
    "person_pic = Image.open(person_pic_path).convert('RGBA').resize((100, 100))\n",
    "\n",
    "# Save resized images as files\n",
    "bot_pic_file = 'bot_pic_resized.jpg'\n",
    "person_pic_file = 'person_pic_resized.png'\n",
    "\n",
    "# Save the resized images\n",
    "bot_pic.save(bot_pic_file)\n",
    "person_pic.save(person_pic_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_fields():\n",
    "    return \"\", None\n",
    "\n",
    "pc = Pinecone(api_key=\"YOUR_PINECONE_API_KEY\")\n",
    "index_name = \"finantial-data\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store = Pinecone(embeddings, index_name=\"finantial-data\")\n",
    "\n",
    "def handle_file_query(file, chat_history=None):\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "    global vector_store\n",
    "\n",
    "    if file == None:\n",
    "        return chat_history + [(\"No file uploaded\", \"Please upload a file\")]\n",
    "\n",
    "    with open(file.name, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "        # Get the number of pages\n",
    "        num_pages = len(reader.pages)\n",
    "\n",
    "        queries_pdf = ''\n",
    "        # Read each page\n",
    "        for page in range(num_pages):\n",
    "            text = reader.pages[page].extract_text()\n",
    "            queries_pdf += text\n",
    "        queries_from_pdf = queries_pdf.split('\\n')\n",
    "        response_from_file = chat_history.copy()  # Copy the existing chat history\n",
    "        for user_query in queries_from_pdf:\n",
    "            if user_query:\n",
    "              results = vector_store.similarity_search(query=i, k=1)\n",
    "\n",
    "              # Create a prompt template\n",
    "              prompt_template = PromptTemplate(\n",
    "                  input_variables=[\"results\", \"query\"],\n",
    "                  template='''Given the data: {results}, please respond to the query: \"{query}\".'''\n",
    "              )\n",
    "\n",
    "              # Format the prompt\n",
    "              formatted_prompt = prompt_template.format(results=results, query=user_query)\n",
    "\n",
    "              # Create messages to send to the chat model\n",
    "              messages = [{\"role\": \"human\", \"content\": formatted_prompt}]\n",
    "\n",
    "              # Invoke the chat model\n",
    "              response = chat.invoke(messages).content\n",
    "              print(user_query, response)\n",
    "              response_from_file.append((i, f\"Response to {response}\"))\n",
    "\n",
    "        return response_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(\n",
    "    index='finantial-data',\n",
    "    embedding=embeddings,\n",
    "    text_key=\"text\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\")\n",
    "\n",
    "def truncate_to_token_limit(text, max_tokens):\n",
    "    \"\"\"Truncate text to fit within token limit\"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "        text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "def get_token_count(text):\n",
    "    \"\"\"Get the number of tokens in a text\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def handle_query(user_query, chat_history):\n",
    "    # Calculate tokens for the base prompt template\n",
    "    base_prompt = '''Based on this context: [CONTEXT]\n",
    "    \n",
    "    Please answer the following question: [QUERY]\n",
    "    \n",
    "    If the context doesn't contain enough information to answer the question fully, please state that.'''\n",
    "    \n",
    "    base_tokens = get_token_count(base_prompt.replace('[CONTEXT]', '').replace('[QUERY]', user_query))\n",
    "    \n",
    "    # Calculate remaining tokens for context\n",
    "    available_context_tokens = MAX_INPUT_TOKENS - base_tokens\n",
    "    \n",
    "    # Perform similarity search with the PineconeVectorStore\n",
    "    results = vector_store.similarity_search(\n",
    "        query=user_query,\n",
    "        k=1\n",
    "    )\n",
    "    \n",
    "    # Extract and format the relevant information from results\n",
    "    context = \"\"\n",
    "    for doc in results:\n",
    "        context += str(doc.page_content) + \"\\n\\n\"\n",
    "    \n",
    "    # Truncate context to fit within available tokens\n",
    "    truncated_context = truncate_to_token_limit(context, available_context_tokens)\n",
    "    \n",
    "    # Create a prompt template for responding\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"query\"],\n",
    "        template=base_prompt\n",
    "    )\n",
    "\n",
    "    # Format the prompt\n",
    "    formatted_prompt = prompt_template.format(context=truncated_context, query=user_query)\n",
    "    \n",
    "    # Final check to ensure we're within limits\n",
    "    final_token_count = get_token_count(formatted_prompt)\n",
    "    if final_token_count > MAX_INPUT_TOKENS:\n",
    "        raise ValueError(f\"Token count ({final_token_count}) exceeds maximum input tokens ({MAX_INPUT_TOKENS})\")\n",
    "    \n",
    "    # Create messages to send to the chat model\n",
    "    messages = [{\"role\": \"human\", \"content\": formatted_prompt}]\n",
    "\n",
    "    # Invoke the chat model to generate a response\n",
    "    response = chat.invoke(messages).content\n",
    "    chat_history.append((user_query, response))\n",
    "    return response, chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as chatbot_app:\n",
    "    # gr.Markdown(\"\")\n",
    "    gr.Markdown(\"Restaurant Chatbot\")\n",
    "    # gr.Markdown(\"Welcome! You‚Äôre interacting with an exciting restaurant chatbot designed to enhance your dining experience. Whether you have questions about our menu, want to make a reservation, or need recommendations, I‚Äôm here to help!\")\n",
    "    gr.Markdown(\"Welcome!\")\n",
    "\n",
    "    chatbot = gr.Chatbot(\n",
    "        [],\n",
    "        elem_id=\"chatbot\",\n",
    "        bubble_full_width=False,\n",
    "        height=400,\n",
    "        avatar_images=(person_pic_file, bot_pic_file),\n",
    "        value=[]\n",
    "    )\n",
    "\n",
    "\n",
    "    prompt = gr.Textbox(placeholder=\"Message Chatbot\", interactive=True, label=None, show_label=False, lines=1, max_lines=3)\n",
    "\n",
    "    with gr.Row(equal_height=False):\n",
    "        submit_btn = gr.Button(\"Submit\", scale=1)\n",
    "        upload_btn = gr.UploadButton(\"üìÅ Upload PDF or doc files\", file_types=['.pdf', '.doc'], file_count=\"single\")\n",
    "        clear_btn = gr.Button(\"Clear\", scale=1)\n",
    "\n",
    "    gr.on(\n",
    "        triggers=[submit_btn.click, prompt.submit],\n",
    "        fn=handle_query,\n",
    "        inputs=[prompt, chatbot],\n",
    "        outputs=[prompt, chatbot],\n",
    "        queue=False\n",
    "    )\n",
    "\n",
    "    upload_btn.upload(fn=handle_file_query, inputs=[upload_btn, chatbot], outputs=chatbot)\n",
    "\n",
    "    clear_btn.click(fn=clear_fields, outputs=[prompt, upload_btn])\n",
    "\n",
    "chatbot_app.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
